from pyspark.sql import SparkSession


# --class org.apache.hudi.utilities.streamer.HoodieStreamer \
# --packages org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0 \
# --properties-file config/spark-config.properties \
# --master 'local[*]' \
# --executor-memory 1g \
# utilities-jar/hudi-utilities-slim-bundle_2.12-0.14.0.jar \
# --table-type COPY_ON_WRITE \
# --target-base-path file:///Users/rahul/PythonWorkSpace/datalake/hudidb/  \
# --target-table retail_transactions \
# --source-ordering-field tran_date \
# --source-class org.apache.hudi.utilities.sources.debezium.PostgresDebeziumSource \
# --payload-class org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload \
# --op UPSERT \
# --continuous \
# --source-limit 4000000 \
# --min-sync-interval-seconds 20 \
# --hoodie-conf bootstrap.servers=localhost:9092 \
# --hoodie-conf schema.registry.url=http://localhost:8081 \
# --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=http://localhost:8081/subjects/test1.v1.retail_transactions-value/versions/latest \
# --hoodie-conf hoodie.deltastreamer.source.kafka.value.deserializer.class=io.confluent.kafka.serializers.KafkaAvroDeserializer \
# --hoodie-conf hoodie.deltastreamer.source.kafka.topic=test1.v1.retail_transactions \
# --hoodie-conf auto.offset.reset=earliest \
# --hoodie-conf hoodie.datasource.write.recordkey.field=tran_id \
# --hoodie-conf hoodie.datasource.write.partitionpath.field=store_city \
# --hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator \
# --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \
# --hoodie-conf hoodie.datasource.write.precombine.field=tran_date

spark = SparkSession.builder \
    .appName("Hudi Basics") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("fs.s3a.signing-algorithm", "S3SignerType") \
    .getOrCreate()

spark



spark.stop()


df = spark.read.csv("s3a://kevin/combined_financial_data_idx.csv", header=True, inferSchema=True)


# pyspark
columns = ["ts","uuid","rider","driver","fare","city"]
data =[(1695159649087,"334e26e9-8355-45cc-97c6-c31daf0df330","rider-A","driver-K",19.10,"san_francisco"),
       (1695091554788,"e96c4396-3fad-413a-a942-4cb36106d721","rider-C","driver-M",27.70 ,"san_francisco"),
       (1695046462179,"9909a8b1-2d15-4d3d-8ec9-efc48c536a00","rider-D","driver-L",33.90 ,"san_francisco"),
       (1695516137016,"e3cf430c-889d-4015-bc98-59bdce1e530c","rider-F","driver-P",34.15,"sao_paulo"),
       (1695115999911,"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa","rider-J","driver-T",17.85,"chennai")]
inserts = spark.createDataFrame(data).toDF(*columns)
inserts.show()



hudi_options = {
    'hoodie.table.name': tableName,
    'hoodie.datasource.write.partitionpath.field': 'city'
}



inserts.write.format("hudi"). \
    options(**hudi_options). \
    mode("overwrite"). \
    save(basePath)





# pyspark
tripsDF = spark.read.format("hudi").load(basePath)
tripsDF.createOrReplaceTempView("trips_table")

spark.sql("SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0").show()
spark.sql("SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table").show()
