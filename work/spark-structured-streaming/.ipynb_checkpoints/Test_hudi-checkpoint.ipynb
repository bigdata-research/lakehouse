{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f091525f-4821-41c0-ae9b-d1275e02e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 09:02:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/14 09:02:46 WARN SparkSession: Cannot use org.apache.spark.sql.hudi.HoodieSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/14 09:02:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4a9de221ea83:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Hudi Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a5c938bf850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# --class org.apache.hudi.utilities.streamer.HoodieStreamer \\\n",
    "# --packages org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0 \\\n",
    "# --properties-file config/spark-config.properties \\\n",
    "# --master 'local[*]' \\\n",
    "# --executor-memory 1g \\\n",
    "# utilities-jar/hudi-utilities-slim-bundle_2.12-0.14.0.jar \\\n",
    "# --table-type COPY_ON_WRITE \\\n",
    "# --target-base-path file:///Users/rahul/PythonWorkSpace/datalake/hudidb/  \\\n",
    "# --target-table retail_transactions \\\n",
    "# --source-ordering-field tran_date \\\n",
    "# --source-class org.apache.hudi.utilities.sources.debezium.PostgresDebeziumSource \\\n",
    "# --payload-class org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload \\\n",
    "# --op UPSERT \\\n",
    "# --continuous \\\n",
    "# --source-limit 4000000 \\\n",
    "# --min-sync-interval-seconds 20 \\\n",
    "# --hoodie-conf bootstrap.servers=localhost:9092 \\\n",
    "# --hoodie-conf schema.registry.url=http://localhost:8081 \\\n",
    "# --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=http://localhost:8081/subjects/test1.v1.retail_transactions-value/versions/latest \\\n",
    "# --hoodie-conf hoodie.deltastreamer.source.kafka.value.deserializer.class=io.confluent.kafka.serializers.KafkaAvroDeserializer \\\n",
    "# --hoodie-conf hoodie.deltastreamer.source.kafka.topic=test1.v1.retail_transactions \\\n",
    "# --hoodie-conf auto.offset.reset=earliest \\\n",
    "# --hoodie-conf hoodie.datasource.write.recordkey.field=tran_id \\\n",
    "# --hoodie-conf hoodie.datasource.write.partitionpath.field=store_city \\\n",
    "# --hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n",
    "# --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n",
    "# --hoodie-conf hoodie.datasource.write.precombine.field=tran_date\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hudi Basics\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.signing-algorithm\", \"S3SignerType\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n",
    "\n",
    "# --packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.0,org.apache.hadoop:hadoop-aws:3.3.4 \\\n",
    "# --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n",
    "# --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n",
    "# --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n",
    "# --conf 'spark.hadoop.fs.s3a.access.key=<your-MinIO-access-key>' \\\n",
    "# --conf 'spark.hadoop.fs.s3a.secret.key=<your-MinIO-secret-key>'\\\n",
    "# --conf 'spark.hadoop.fs.s3a.endpoint=<your-MinIO-IP>:9000' \\\n",
    "# --conf 'spark.hadoop.fs.s3a.path.style.access=' \\\n",
    "# --conf 'fs.s3a.signing-algorithm=S3SignerType'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539a7867-63a1-400c-9d26-ca39b473bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7038a3-5685-4519-bcd6-479d77bb8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "columns = [\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\"]\n",
    "data =[(1695159649087,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n",
    "       (1695091554788,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-C\",\"driver-M\",27.70 ,\"san_francisco\"),\n",
    "       (1695046462179,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-D\",\"driver-L\",33.90 ,\"san_francisco\"),\n",
    "       (1695516137016,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-F\",\"driver-P\",34.15,\"sao_paulo\"),\n",
    "       (1695115999911,\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\",\"rider-J\",\"driver-T\",17.85,\"chennai\")]\n",
    "inserts = spark.createDataFrame(data).toDF(*columns)\n",
    "inserts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42379a83-b9c7-4752-be4d-fb782e468a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    'hoodie.table.name': tableName,\n",
    "    'hoodie.datasource.write.partitionpath.field': 'city'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35698f3-d705-4bbf-90e5-0baa503e94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inserts.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8e489-d417-48b8-960a-aeffabc7954c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd25eb6-a2f3-4c0d-9480-64f08f235615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "tripsDF = spark.read.format(\"hudi\").load(basePath)\n",
    "tripsDF.createOrReplaceTempView(\"trips_table\")\n",
    "\n",
    "spark.sql(\"SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0\").show()\n",
    "spark.sql(\"SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
